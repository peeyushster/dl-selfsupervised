{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm, trange\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "matplotlib.rcParams['figure.figsize'] = [12,12]\n",
    "matplotlib.rcParams['figure.dpi'] = 200\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "\n",
    "from data_helper import LabeledDataset\n",
    "from helper import collate_fn, draw_box\n",
    "\n",
    "import itertools\n",
    "from scipy.spatial.distance import cdist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(0)\n",
    "np.random.seed(0)\n",
    "torch.manual_seed(0);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All the images are saved in image_folder\n",
    "# All the labels are saved in the annotation_csv file\n",
    "image_folder = '../data'\n",
    "annotation_csv = '../data/annotation.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You shouldn't change the unlabeled_scene_index\n",
    "# The first 106 scenes are unlabeled\n",
    "unlabeled_scene_index = np.arange(106)\n",
    "# The scenes from 106 - 133 are labeled\n",
    "# You should devide the labeled_scene_index into two subsets (training and validation)\n",
    "labeled_scene_index = np.arange(106, 134)\n",
    "\n",
    "train_split=int(106*0.8)\n",
    "train_inds = unlabeled_scene_index[:train_split]\n",
    "val_inds = unlabeled_scene_index[train_split:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_SAMPLE_PER_SCENE = 126\n",
    "NUM_IMAGE_PER_SAMPLE = 6\n",
    "image_names = [\n",
    "    'CAM_FRONT_LEFT.jpeg',\n",
    "    'CAM_FRONT.jpeg',\n",
    "    'CAM_FRONT_RIGHT.jpeg',\n",
    "    'CAM_BACK_LEFT.jpeg',\n",
    "    'CAM_BACK.jpeg',\n",
    "    'CAM_BACK_RIGHT.jpeg',\n",
    "    ]\n",
    "\n",
    "def rgb_jittering(im):\n",
    "    im = np.array(im, 'int32')\n",
    "    for ch in range(3):\n",
    "        im[:, :, ch] += np.random.randint(-2, 2)\n",
    "    im[im > 255] = 255\n",
    "    im[im < 0] = 0\n",
    "    return im.astype('uint8')\n",
    "\n",
    "class  JigsawDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self,  image_folder, scene_index, first_dim):\n",
    "        \n",
    "        self.image_folder = image_folder\n",
    "        self.scene_index = scene_index\n",
    "        self.permutations = self.get_permutations()\n",
    "        \n",
    "        self.__augment_tile = transforms.Compose([\n",
    "            transforms.RandomCrop((256,256)),\n",
    "            transforms.Lambda(rgb_jittering),\n",
    "            transforms.ToTensor(),\n",
    "        ])\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        scene_id = self.scene_index[index // NUM_SAMPLE_PER_SCENE]\n",
    "        sample_id = index % NUM_SAMPLE_PER_SCENE\n",
    "        sample_path = os.path.join(self.image_folder, f'scene_{scene_id}', f'sample_{sample_id}')\n",
    "\n",
    "        tiles = [None] * 6\n",
    "        for n in range(6):\n",
    "            image_path = os.path.join(sample_path, image_names[n])\n",
    "            tile = Image.open(image_path)\n",
    "            tile = self.__augment_tile(tile)\n",
    "            \n",
    "            # Normalize the patches independently to avoid low level features shortcut\n",
    "            m, s = tile.view(3, -1).mean(dim=1).numpy(), tile.view(3, -1).std(dim=1).numpy()\n",
    "            s[s == 0] = 1\n",
    "            norm = transforms.Normalize(mean=m.tolist(), std=s.tolist())\n",
    "            tile = norm(tile)\n",
    "            tiles[n] = tile\n",
    "       \n",
    "        order = np.random.randint(len(self.permutations))\n",
    "        data = [tiles[self.permutations[order][t]] for t in range(6)]\n",
    "        data = torch.stack(data, 0)\n",
    "\n",
    "        return data, int(order)#, torch.stack(tiles)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.scene_index.size * NUM_SAMPLE_PER_SCENE\n",
    "    \n",
    "    def get_permutations(self, classes=500, selection=\"max\"):\n",
    "        P_hat = np.array(list(itertools.permutations(list(range(6)), 6)))\n",
    "        n = P_hat.shape[0]\n",
    "\n",
    "        for i in trange(classes):\n",
    "            if i==0:\n",
    "                j = np.random.randint(n)\n",
    "                P = np.array(P_hat[j]).reshape([1,-1])\n",
    "            else:\n",
    "                P = np.concatenate([P,P_hat[j].reshape([1,-1])],axis=0)\n",
    "\n",
    "            P_hat = np.delete(P_hat,j,axis=0)\n",
    "            D = cdist(P,P_hat, metric='hamming').mean(axis=0).flatten()\n",
    "\n",
    "            if selection=='max':\n",
    "                j = D.argmax()\n",
    "            else:\n",
    "                m = int(D.shape[0]/2)\n",
    "                S = D.argsort()\n",
    "                j = S[np.random.randint(m-10,m+10)]\n",
    "\n",
    "        return P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unlabeled_trainset = JigsawDataset(image_folder=image_folder, scene_index=train_inds, first_dim='sample')\n",
    "unlabeled_valset = JigsawDataset(image_folder=image_folder, scene_index=val_inds, first_dim='sample')\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(unlabeled_trainset, batch_size=32, pin_memory=True,shuffle=True, num_workers=2)\n",
    "valloader = torch.utils.data.DataLoader(unlabeled_valset, batch_size=1, pin_memory=True,shuffle=True, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class JigsawNet(nn.Module):\n",
    "    def ConvBlock(self, in_channels, out_channels, kernel_size = 3, stride = 1, padding = 1, use_bias = False):\n",
    "        block = nn.Sequential(nn.Conv2d(in_channels, out_channels, kernel_size, \n",
    "                                        stride, padding, bias = use_bias),\n",
    "                              nn.BatchNorm2d(out_channels),\n",
    "                              nn.ReLU(True)\n",
    "                             )\n",
    "        return block\n",
    "    \n",
    "    def Bridge(self, in_channels, out_channels):\n",
    "        bridge = nn.Sequential(self.ConvBlock(in_channels, out_channels),\n",
    "                               self.ConvBlock(out_channels, out_channels)\n",
    "                              )\n",
    "        return bridge\n",
    "        \n",
    "        \n",
    "    def __init__(self, classes = 500, encoder='resnet34', pretrained = False, depth = 6):\n",
    "        '''\n",
    "        num_classes: Number of channels/classes for segmentation\n",
    "        output_size: Final output size of the image (H*H)\n",
    "        encoder: Supports resnet18, resnet 34 and resnet50 architectures\n",
    "        pretrained: For loading a pretrained resnet model as encoder\n",
    "        '''\n",
    "        super(JigsawNet,self).__init__()  \n",
    "        self.depth = depth        \n",
    "        self.resnet = torchvision.models.resnet50(pretrained=pretrained) if encoder == \"resnet50\" else\\\n",
    "                            torchvision.models.resnet34(pretrained=pretrained) if encoder == \"resnet34\" else\\\n",
    "                            torchvision.models.resnet18(pretrained=pretrained)\n",
    "        \n",
    "        self.resnet_layers = list(self.resnet.children())\n",
    "        self.n = 2048 if encoder == \"resnet50\" else 512\n",
    "        \n",
    "        self.input_block = nn.Sequential(*self.resnet_layers)[:3]\n",
    "        #self.input_block[0] = nn.Conv2d(18, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
    "        self.input_pool = self.resnet_layers[3]\n",
    "        self.down_blocks = nn.ModuleList([i for i in self.resnet_layers if isinstance(i, nn.Sequential)])\n",
    "\n",
    "        self.bridge = self.Bridge(self.n, self.n)\n",
    "        self.down_channels = nn.Conv2d(512,32,kernel_size=1,stride=1)\n",
    "        self.fc6 = nn.Sequential()\n",
    "        self.fc6.add_module('fc6_s1',nn.Linear(32*8*8, 1024))\n",
    "        self.fc6.add_module('relu6_s1',nn.ReLU(inplace=True))\n",
    "        self.fc6.add_module('drop6_s1',nn.Dropout(p=0.5))\n",
    "\n",
    "        self.fc7 = nn.Sequential()\n",
    "        self.fc7.add_module('fc7',nn.Linear(6*1024,4096))\n",
    "        self.fc7.add_module('relu7',nn.ReLU(inplace=True))\n",
    "        self.fc7.add_module('drop7',nn.Dropout(p=0.5))\n",
    "\n",
    "        self.classifier = nn.Sequential()\n",
    "        self.classifier.add_module('fc8',nn.Linear(4096, classes))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        B = x.shape[0]\n",
    "        x = x.transpose(0,1)\n",
    "        \n",
    "        x_list = []\n",
    "        for i in range(6):\n",
    "            z = self.input_pool(self.input_block(x[i]))\n",
    "            for block in self.down_blocks:\n",
    "                z = block(z)\n",
    "            z = self.down_channels(z)\n",
    "            z = self.fc6(z.view(B,-1))\n",
    "            z = z.view([B,1,-1])\n",
    "            x_list.append(z)\n",
    "            \n",
    "        del z            \n",
    "        x = torch.cat(x_list,1)\n",
    "        del x_list\n",
    "        x = self.fc7(x.view(B,-1))\n",
    "        x = self.classifier(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.0005\n",
    "momentum = 0.9\n",
    "num_epochs = 50\n",
    "weight_decay = 5e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = JigsawNet(classes=500, encoder=\"resnet34\", pretrained = False).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(),lr=lr,momentum=momentum,weight_decay = weight_decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "best_val_loss = 1000000\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss = 0\n",
    "    model.train()\n",
    "    for i, (sample, target) in enumerate(tqdm(trainloader)):        \n",
    "        sample, target = sample.to(device), target.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        out = model(sample)\n",
    "\n",
    "        loss = criterion(out, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += loss.item()\n",
    "        if (i+1)%150 == 0:\n",
    "            print(\"Epoch: {} | Iter: {} | Train loss: {}\".format(epoch+1, i+1, train_loss/(i+1)))\n",
    "    \n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():        \n",
    "        for i, (sample,target) in enumerate(tqdm(valloader)):\n",
    "            sample, target = sample.to(device),target.to(device)\n",
    "            out = model(sample)\n",
    "            loss = criterion(out, target)\n",
    "            val_loss += loss.item()\n",
    "    \n",
    "    epoch_val_loss = val_loss/len(valloader)\n",
    "    print(\"Epoch: {} | Val loss: {}\".format(epoch+1,epoch_val_loss))\n",
    "    if epoch_val_loss<best_val_loss:\n",
    "        best_val_loss = epoch_val_loss\n",
    "        print(\"Saving model...\")\n",
    "        torch.save(model.state_dict(),'jigsaw_task_001.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
