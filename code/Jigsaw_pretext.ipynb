{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm, trange\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "matplotlib.rcParams['figure.figsize'] = [12,12]\n",
    "matplotlib.rcParams['figure.dpi'] = 200\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "\n",
    "from data_helper import LabeledDataset\n",
    "from helper import collate_fn, draw_box\n",
    "\n",
    "import itertools\n",
    "from scipy.spatial.distance import cdist\n",
    "from helper import compute_ats_bounding_boxes, compute_ts_road_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(0)\n",
    "np.random.seed(0)\n",
    "torch.manual_seed(0);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All the images are saved in image_folder\n",
    "# All the labels are saved in the annotation_csv file\n",
    "image_folder = '../data'\n",
    "annotation_csv = '../data/annotation.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You shouldn't change the unlabeled_scene_index\n",
    "# The first 106 scenes are unlabeled\n",
    "unlabeled_scene_index = np.arange(106)\n",
    "# The scenes from 106 - 133 are labeled\n",
    "# You should devide the labeled_scene_index into two subsets (training and validation)\n",
    "labeled_scene_index = np.arange(106, 134)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Labeled Dataloader "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AddGaussianNoise(object):\n",
    "    def __init__(self, mean=0., std=1.):\n",
    "        self.std = std\n",
    "        self.mean = mean\n",
    "        \n",
    "    def __call__(self, tensor):\n",
    "        return tensor + torch.randn(tensor.size()) * self.std + self.mean\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__ + '(mean={0}, std={1})'.format(self.mean, self.std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled_scene_index = np.arange(106, 134)\n",
    "train_inds = labeled_scene_index[:23]\n",
    "val_inds = labeled_scene_index[23:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_train = transforms.Compose([torchvision.transforms.Resize((256,256)),\n",
    "                                transforms.ToTensor(), \n",
    "                                transforms.Normalize([ 0.485, 0.456, 0.406 ],[ 0.229, 0.224, 0.225 ]), \n",
    "                                AddGaussianNoise(0., 0.1)])\n",
    "transform_val = transforms.Compose([torchvision.transforms.Resize((256,256)),\n",
    "                                transforms.ToTensor(), \n",
    "                                transforms.Normalize([ 0.485, 0.456, 0.406 ],[ 0.229, 0.224, 0.225 ])])\n",
    "\n",
    "labeled_trainset = LabeledDataset(image_folder=image_folder,\n",
    "                                  annotation_file=annotation_csv,\n",
    "                                  scene_index=train_inds,\n",
    "                                  transform=transform_train,\n",
    "                                  extra_info=True\n",
    "                                 )\n",
    "labeled_valset = LabeledDataset(image_folder=image_folder,\n",
    "                                  annotation_file=annotation_csv,\n",
    "                                  scene_index=val_inds,\n",
    "                                  transform=transform_val,\n",
    "                                  extra_info=True\n",
    "                                 )\n",
    "labeled_trainloader = torch.utils.data.DataLoader(labeled_trainset, batch_size=16, shuffle=True, num_workers=2, collate_fn=collate_fn)\n",
    "labeled_valloader = torch.utils.data.DataLoader(labeled_valset, batch_size=1,shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unlabeled Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_SAMPLE_PER_SCENE = 126\n",
    "NUM_IMAGE_PER_SAMPLE = 6\n",
    "image_names = [\n",
    "    'CAM_FRONT_LEFT.jpeg',\n",
    "    'CAM_FRONT.jpeg',\n",
    "    'CAM_FRONT_RIGHT.jpeg',\n",
    "    'CAM_BACK_LEFT.jpeg',\n",
    "    'CAM_BACK.jpeg',\n",
    "    'CAM_BACK_RIGHT.jpeg',\n",
    "    ]\n",
    "\n",
    "def rgb_jittering(im):\n",
    "    im = np.array(im, 'int32')\n",
    "    for ch in range(3):\n",
    "        im[:, :, ch] += np.random.randint(-2, 2)\n",
    "    im[im > 255] = 255\n",
    "    im[im < 0] = 0\n",
    "    return im.astype('uint8')\n",
    "\n",
    "class  JigsawDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self,  image_folder, scene_index, first_dim):\n",
    "        \n",
    "        self.image_folder = image_folder\n",
    "        self.scene_index = scene_index\n",
    "        self.permutations = self.get_permutations()\n",
    "        \n",
    "        self.__augment_tile = transforms.Compose([\n",
    "            transforms.RandomCrop((256,256)),\n",
    "            transforms.Lambda(rgb_jittering),\n",
    "            transforms.ToTensor(),\n",
    "        ])\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        scene_id = self.scene_index[index // NUM_SAMPLE_PER_SCENE]\n",
    "        sample_id = index % NUM_SAMPLE_PER_SCENE\n",
    "        sample_path = os.path.join(self.image_folder, f'scene_{scene_id}', f'sample_{sample_id}')\n",
    "\n",
    "        tiles = [None] * 6\n",
    "        for n in range(6):\n",
    "            image_path = os.path.join(sample_path, image_names[n])\n",
    "            tile = Image.open(image_path)\n",
    "            tile = self.__augment_tile(tile)\n",
    "            \n",
    "            # Normalize the patches independently to avoid low level features shortcut\n",
    "            m, s = tile.view(3, -1).mean(dim=1).numpy(), tile.view(3, -1).std(dim=1).numpy()\n",
    "            s[s == 0] = 1\n",
    "            norm = transforms.Normalize(mean=m.tolist(), std=s.tolist())\n",
    "            tile = norm(tile)\n",
    "            tiles[n] = tile\n",
    "       \n",
    "        order = np.random.randint(len(self.permutations))\n",
    "        data = [tiles[self.permutations[order][t]] for t in range(6)]\n",
    "        data = torch.stack(data, 0)\n",
    "\n",
    "        return data, int(order)#, torch.stack(tiles)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.scene_index.size * NUM_SAMPLE_PER_SCENE\n",
    "    \n",
    "    def get_permutations(self, classes=100, selection=\"max\"):\n",
    "        P_hat = np.array(list(itertools.permutations(list(range(6)), 6)))\n",
    "        n = P_hat.shape[0]\n",
    "\n",
    "        for i in trange(classes):\n",
    "            if i==0:\n",
    "                j = np.random.randint(n)\n",
    "                P = np.array(P_hat[j]).reshape([1,-1])\n",
    "            else:\n",
    "                P = np.concatenate([P,P_hat[j].reshape([1,-1])],axis=0)\n",
    "\n",
    "            P_hat = np.delete(P_hat,j,axis=0)\n",
    "            D = cdist(P,P_hat, metric='hamming').mean(axis=0).flatten()\n",
    "\n",
    "            if selection=='max':\n",
    "                j = D.argmax()\n",
    "            else:\n",
    "                m = int(D.shape[0]/2)\n",
    "                S = D.argsort()\n",
    "                j = S[np.random.randint(m-10,m+10)]\n",
    "\n",
    "        return P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:00<00:00, 2777.96it/s]\n"
     ]
    }
   ],
   "source": [
    "unlabeled_trainset = JigsawDataset(image_folder=image_folder, scene_index=unlabeled_scene_index, first_dim='sample')\n",
    "\n",
    "unlabeled_trainloader = torch.utils.data.DataLoader(unlabeled_trainset, batch_size=16, pin_memory=True,shuffle=True, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def ConvBlock(self, in_channels, out_channels, kernel_size = 3, stride = 1, padding = 1, use_bias = False):\n",
    "        block = nn.Sequential(nn.Conv2d(in_channels, out_channels, kernel_size, \n",
    "                                        stride, padding, bias = use_bias),\n",
    "                              nn.BatchNorm2d(out_channels),\n",
    "                              nn.ReLU(True)\n",
    "                             )\n",
    "        return block\n",
    "    \n",
    "    def Bridge(self, in_channels, out_channels):\n",
    "        bridge = nn.Sequential(self.ConvBlock(in_channels, out_channels),\n",
    "                               self.ConvBlock(out_channels, out_channels)\n",
    "                              )\n",
    "        return bridge\n",
    "        \n",
    "        \n",
    "    def __init__(self, encoder='resnet34', pretrained = False, depth = 6):\n",
    "        '''\n",
    "        num_classes: Number of channels/classes for segmentation\n",
    "        output_size: Final output size of the image (H*H)\n",
    "        encoder: Supports resnet18, resnet 34 and resnet50 architectures\n",
    "        pretrained: For loading a pretrained resnet model as encoder\n",
    "        '''\n",
    "        super(Encoder,self).__init__()  \n",
    "        self.depth = depth        \n",
    "        self.resnet = torchvision.models.resnet50(pretrained=pretrained) if encoder == \"resnet50\" else\\\n",
    "                            torchvision.models.resnet34(pretrained=pretrained) if encoder == \"resnet34\" else\\\n",
    "                            torchvision.models.resnet18(pretrained=pretrained)\n",
    "        \n",
    "        self.resnet_layers = list(self.resnet.children())\n",
    "        self.n = 2048 if encoder == \"resnet50\" else 512\n",
    "        \n",
    "        self.input_block = nn.Sequential(*self.resnet_layers)[:3]\n",
    "        #self.input_block[0] = nn.Conv2d(18, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
    "        self.input_pool = self.resnet_layers[3]\n",
    "        self.down_blocks = nn.ModuleList([i for i in self.resnet_layers if isinstance(i, nn.Sequential)])\n",
    "\n",
    "        self.bridge = self.Bridge(self.n, self.n)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        B = x.shape[0]\n",
    "        x = x.transpose(0,1)\n",
    "        \n",
    "        h_list = []\n",
    "        z_list = []\n",
    "        for i in range(6):\n",
    "            z = self.input_pool(self.input_block(x[i]))\n",
    "            for block in self.down_blocks:\n",
    "                z = block(z)\n",
    "            z = self.bridge(z)\n",
    "            z_list.append(z)\n",
    "#             h = self.pool(z)\n",
    "#             h = h.view([B,1,-1])\n",
    "#             h_list.append(h)\n",
    "            \n",
    "#         del h   \n",
    "        del z\n",
    "#         h = torch.cat(h_list,1)\n",
    "        z = torch.cat(z_list,1)\n",
    "#         del h_list\n",
    "        del z_list\n",
    "        return z #b,512*6,8,8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class JigsawNet(nn.Module):\n",
    "    def __init__(self, classes = 500):\n",
    "        super(JigsawNet,self).__init__()  \n",
    "        self.pool = nn.AdaptiveAvgPool2d(output_size=(1,1))\n",
    "        self.fc = nn.Sequential()\n",
    "        self.fc.add_module('fc7',nn.Linear(6*512,512))\n",
    "        self.fc.add_module('relu7',nn.ReLU(inplace=True))\n",
    "        self.fc.add_module('drop7',nn.Dropout(p=0.5))\n",
    "\n",
    "        self.classifier = nn.Sequential()\n",
    "        self.classifier.add_module('fc8',nn.Linear(512, classes))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        B = x.shape[0]\n",
    "        h = self.pool(x)\n",
    "        x = self.fc(h.view(B,-1))\n",
    "        x = self.classifier(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def ConvBlock(self, in_channels, out_channels, kernel_size = 3, stride = 1, padding = 1, use_bias = False):\n",
    "        block = nn.Sequential(nn.Conv2d(in_channels, out_channels, kernel_size, \n",
    "                                        stride, padding, bias = use_bias),\n",
    "                              nn.BatchNorm2d(out_channels),\n",
    "                              nn.ReLU(True)\n",
    "                             )\n",
    "        return block\n",
    "    \n",
    "    def Bridge(self, in_channels, out_channels):\n",
    "        bridge = nn.Sequential(self.ConvBlock(in_channels, out_channels),\n",
    "                               self.ConvBlock(out_channels, out_channels)\n",
    "                              )\n",
    "        return bridge\n",
    "    \n",
    "    def UpsampleBlock(self, in_channels, out_channels, use_bias=False):\n",
    "        upsample = nn.Sequential(nn.ConvTranspose2d(in_channels, out_channels, kernel_size=2, stride=2, bias=use_bias),\n",
    "                                 nn.BatchNorm2d(out_channels),\n",
    "                                 nn.ReLU(True))\n",
    "        return upsample\n",
    "        \n",
    "    def UpsampleConv(self, in_channels, out_channels):\n",
    "        upsample_conv = nn.Sequential(self.ConvBlock(in_channels, out_channels),\n",
    "                                      self.ConvBlock(out_channels, out_channels))    \n",
    "        return upsample_conv\n",
    "        \n",
    "        \n",
    "    def __init__(self, classes = 1, depth = 6, output_size=(800,800)):\n",
    "        '''\n",
    "        num_classes: Number of channels/classes for segmentation\n",
    "        output_size: Final output size of the image (H*H)\n",
    "        encoder: Supports resnet18, resnet 34 and resnet50 architectures\n",
    "        pretrained: For loading a pretrained resnet model as encoder\n",
    "        '''\n",
    "        super(Decoder,self).__init__()  \n",
    "        self.depth = depth        \n",
    "        self.num_classes = classes\n",
    "        self.output_size = output_size\n",
    "        self.n = 512*6\n",
    "        \n",
    "        self.up_blocks = nn.ModuleList([self.UpsampleBlock(self.n,self.n//2)[0],\n",
    "                                        self.UpsampleBlock(self.n//2,self.n//4)[0],\n",
    "                                        self.UpsampleBlock(self.n//4,self.n//8)[0],\n",
    "                                        self.UpsampleBlock(self.n//8,self.n//16)[0],\n",
    "                                        self.UpsampleBlock(self.n//16,self.n//32)[0]])\n",
    "        \n",
    "        self.up_conv = nn.ModuleList([self.UpsampleConv(self.n//2,self.n//2),\n",
    "                                      self.UpsampleConv(self.n//4,self.n//4),\n",
    "                                      self.UpsampleConv(self.n//8,self.n//8),\n",
    "                                      self.UpsampleConv(self.n//16 ,self.n//16),\n",
    "                                      self.UpsampleConv(self.n//32,self.n//32)])\n",
    "        \n",
    "        self.final_upsample_1 = self.UpsampleBlock(self.n//32,self.n//64)\n",
    "        self.final_upsample_2 = self.UpsampleBlock(self.n//64,self.num_classes)[0]\n",
    "        \n",
    "        self.final_pooling = nn.AdaptiveMaxPool2d(output_size=self.output_size)\n",
    "        \n",
    "    def forward(self, z):\n",
    "        for i, block in enumerate(self.up_blocks):          \n",
    "            z = block(z)\n",
    "            z = self.up_conv[i](z)\n",
    "        x = self.final_upsample_1(z)#.unsqueeze(0))\n",
    "        del z\n",
    "        x = self.final_upsample_2(x)\n",
    "        x = self.final_pooling(x)\n",
    "        x = x.view(-1,self.output_size,self.output_size)\n",
    "        #x = torch.sigmoid(x)\n",
    "        return x "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.0005\n",
    "momentum = 0.9\n",
    "num_epochs = 50\n",
    "weight_decay = 5e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "jigsawnet = JigsawNet(classes=100).to(device)\n",
    "encoder = Encoder(encoder='resnet34', pretrained = False).to(device)\n",
    "decoder = Decoder(classes = 1, output_size=800).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_function_road(x_hat, x):\n",
    "    x_hat_sig = torch.sigmoid(x_hat)\n",
    "    #DICE =  1 - (2*torch.sum(x_hat_sig*x))/(torch.sum(x*x) + torch.sum(x_hat_sig*x_hat_sig))\n",
    "    tp = (x_hat_sig * x).sum()\n",
    "    ts = tp * 1.0 / (x_hat_sig.sum() + x.sum() - tp)\n",
    "    BCE = F.binary_cross_entropy_with_logits(\n",
    "        x_hat, x, reduction='mean'\n",
    "    )\n",
    "    return (-0.3*torch.log(ts)+0.7*BCE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion_jigsaw = nn.CrossEntropyLoss()\n",
    "criterion_road = loss_function_road\n",
    "jigsaw_optimizer = torch.optim.SGD(jigsawnet.parameters(),lr=lr,momentum=momentum,weight_decay = weight_decay)\n",
    "encoder_optimizer = torch.optim.Adam(encoder.parameters(),lr=5e-5)\n",
    "decoder_optimizer = torch.optim.Adam(decoder.parameters(),lr=5e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation(encoder,decoder,valloader,device):\n",
    "    encoder.eval()\n",
    "    decoder.eval()\n",
    "    with torch.no_grad():\n",
    "        tse_road = 0.\n",
    "        total = 0.\n",
    "        for i,(sample, target, road_image,extra) in enumerate(valloader):\n",
    "            total+=1.0  \n",
    "            input_img = sample.to(device)\n",
    "            predicted_road = decoder(encoder(input_img))\n",
    "            predicted_road_map = torch.sigmoid(predicted_road)>0.5\n",
    "            predicted_road_map = (predicted_road_map.squeeze(1)).float()\n",
    "            \n",
    "            tse_road+=compute_ts_road_map(predicted_road_map, road_image.float().to(device))\n",
    "            \n",
    "        return (tse_road/total).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(encoder, \n",
    "          decoder, \n",
    "          jigsawnet, \n",
    "          labeled_trainloader, \n",
    "          unlabeled_trainloader, \n",
    "          valloader, \n",
    "          criterion_jigsaw, \n",
    "          jigsaw_optimizer, \n",
    "          encoder_optimizer, \n",
    "          decoder_optimizer,\n",
    "          criterion_road, \n",
    "          device=device, \n",
    "          c_point=1, \n",
    "          max_iter=400):\n",
    "    labeled_iter = iter(labeled_trainloader)\n",
    "    unlabeled_iter = iter(unlabeled_trainloader)\n",
    "    unlabeled_training_loss = 0\n",
    "    labeled_training_loss = 0\n",
    "    threshold_val = 0\n",
    "    for iterator in tqdm(range(1,max_iter+1)):\n",
    "        \n",
    "        jigsaw_optimizer.zero_grad() \n",
    "        encoder_optimizer.zero_grad() \n",
    "        decoder_optimizer.zero_grad()\n",
    "        \n",
    "        try:\n",
    "            unlabeled_img,permutation_target = next(unlabeled_iter)\n",
    "        except:\n",
    "            unlabeled_iter = iter(unlabeled_trainloader)\n",
    "            unlabeled_img,permutation_target = next(unlabeled_iter,None)\n",
    "        permutation_predicted = jigsawnet(encoder(unlabeled_img.to(device)))\n",
    "        u_loss = criterion_jigsaw(permutation_predicted, permutation_target.to(device))\n",
    "        unlabeled_training_loss+=u_loss.item()\n",
    "        \n",
    "        try:\n",
    "            labeled_img,_,road_image,_ = next(labeled_iter)\n",
    "        except:\n",
    "            labeled_iter = iter(labeled_trainloader)\n",
    "            labeled_img,_,road_image,_ = next(labeled_iter)\n",
    "            \n",
    "        labeled_img = torch.stack(labeled_img).to(device)\n",
    "        road_predicted = decoder(encoder(labeled_img))\n",
    "        l_loss = criterion_road(road_predicted, torch.stack(road_image).float().to(device))\n",
    "        labeled_training_loss+=l_loss.item()\n",
    "        \n",
    "        loss = u_loss+5*l_loss\n",
    "        loss.backward()\n",
    "        \n",
    "        jigsaw_optimizer.step() \n",
    "        encoder_optimizer.step() \n",
    "        decoder_optimizer.step()\n",
    "        \n",
    "        if iterator%c_point==0:\n",
    "            val_tse_road = validation(encoder,decoder,valloader,device)\n",
    "            print('iterator: {}/{} | train loss labeled: {} | train loss unlabeled: {} | val ts road: {}'.format(iterator, \n",
    "                                                                                                                 max_iter,\n",
    "                                                                                                                 round(labeled_training_loss/c_point,2), \n",
    "                                                                                                                 round(unlabeled_training_loss/c_point,2), \n",
    "                                                                                                                 round(val_tse_road,3)))\n",
    "            unlabeled_training_loss = 0\n",
    "            labeled_training_loss = 0\n",
    "            encoder.train()\n",
    "            decoder.train()\n",
    "            \n",
    "            if val_tse_road>threshold_val:\n",
    "                print('--Saving--')\n",
    "                torch.save(encoder.state_dict(),'ss/encoder.pth')\n",
    "                torch.save(decoder.state_dict(),'ss/decoder.pth')\n",
    "                torch.save(jigsawnet.state_dict(),'ss/jigsaw.pth')\n",
    "                threshold_val = val_tse_road\n",
    "                \n",
    "                \n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 9/4000 [00:16<1:56:20,  1.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iterator: 10/4000 | train loss labeled: 0.84 | train loss unlabeled: 4.61 | val ts road: 0.488\n",
      "--Saving--\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 20/4000 [02:15<19:05:10, 17.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iterator: 20/4000 | train loss labeled: 0.64 | train loss unlabeled: 4.6 | val ts road: 0.45\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 29/4000 [02:30<2:31:28,  2.29s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iterator: 30/4000 | train loss labeled: 0.56 | train loss unlabeled: 4.6 | val ts road: 0.697\n",
      "--Saving--\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 40/4000 [04:30<19:20:35, 17.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iterator: 40/4000 | train loss labeled: 0.51 | train loss unlabeled: 4.62 | val ts road: 0.684\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▏         | 50/4000 [05:35<18:15:13, 16.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iterator: 50/4000 | train loss labeled: 0.49 | train loss unlabeled: 4.58 | val ts road: 0.691\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 60/4000 [06:42<18:41:14, 17.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iterator: 60/4000 | train loss labeled: 0.45 | train loss unlabeled: 4.59 | val ts road: 0.694\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 70/4000 [07:47<18:07:03, 16.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iterator: 70/4000 | train loss labeled: 0.44 | train loss unlabeled: 4.63 | val ts road: 0.692\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 80/4000 [08:54<18:45:31, 17.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iterator: 80/4000 | train loss labeled: 0.44 | train loss unlabeled: 4.57 | val ts road: 0.692\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 90/4000 [09:59<18:12:17, 16.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iterator: 90/4000 | train loss labeled: 0.42 | train loss unlabeled: 4.58 | val ts road: 0.668\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▎         | 100/4000 [11:05<18:18:24, 16.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iterator: 100/4000 | train loss labeled: 0.42 | train loss unlabeled: 4.61 | val ts road: 0.669\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 110/4000 [12:12<18:37:32, 17.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iterator: 110/4000 | train loss labeled: 0.39 | train loss unlabeled: 4.6 | val ts road: 0.696\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 120/4000 [13:18<18:06:10, 16.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iterator: 120/4000 | train loss labeled: 0.42 | train loss unlabeled: 4.57 | val ts road: 0.669\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 130/4000 [14:24<18:12:14, 16.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iterator: 130/4000 | train loss labeled: 0.41 | train loss unlabeled: 4.54 | val ts road: 0.676\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 137/4000 [14:36<3:08:07,  2.92s/it] "
     ]
    }
   ],
   "source": [
    "train(encoder, \n",
    "          decoder, \n",
    "          jigsawnet, \n",
    "          labeled_trainloader, \n",
    "          unlabeled_trainloader, \n",
    "          labeled_valloader, \n",
    "          criterion_jigsaw, \n",
    "          jigsaw_optimizer, \n",
    "          encoder_optimizer, \n",
    "          decoder_optimizer,\n",
    "          criterion_road, \n",
    "          device=device, \n",
    "          c_point=10, \n",
    "          max_iter=4000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# best_val_loss = 1000000\n",
    "# for epoch in range(num_epochs):\n",
    "#     train_loss = 0\n",
    "#     model.train()\n",
    "#     for i, (sample, target) in enumerate(tqdm(trainloader)):        \n",
    "#         sample, target = sample.to(device), target.to(device)\n",
    "        \n",
    "#         optimizer.zero_grad()\n",
    "#         out = model(sample)\n",
    "\n",
    "#         loss = criterion(out, target)\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "        \n",
    "#         train_loss += loss.item()\n",
    "#         if (i+1)%150 == 0:\n",
    "#             print(\"Epoch: {} | Iter: {} | Train loss: {}\".format(epoch+1, i+1, train_loss/(i+1)))\n",
    "    \n",
    "#     model.eval()\n",
    "#     val_loss = 0\n",
    "#     with torch.no_grad():        \n",
    "#         for i, (sample,target) in enumerate(tqdm(valloader)):\n",
    "#             sample, target = sample.to(device),target.to(device)\n",
    "#             out = model(sample)\n",
    "#             loss = criterion(out, target)\n",
    "#             val_loss += loss.item()\n",
    "    \n",
    "#     epoch_val_loss = val_loss/len(valloader)\n",
    "#     print(\"Epoch: {} | Val loss: {}\".format(epoch+1,epoch_val_loss))\n",
    "#     if epoch_val_loss<best_val_loss:\n",
    "#         best_val_loss = epoch_val_loss\n",
    "#         print(\"Saving model...\")\n",
    "#         torch.save(model.state_dict(),'jigsaw_task_001.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
