{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "matplotlib.rcParams['figure.figsize'] = [12,12]\n",
    "matplotlib.rcParams['figure.dpi'] = 200\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "\n",
    "from data_helper import UnlabeledDataset, LabeledDataset\n",
    "from helper import collate_fn, draw_box"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(0)\n",
    "np.random.seed(0)\n",
    "torch.manual_seed(0);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All the images are saved in image_folder\n",
    "# All the labels are saved in the annotation_csv file\n",
    "image_folder = '../data'\n",
    "annotation_csv = '../data/annotation.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You shouldn't change the unlabeled_scene_index\n",
    "# The first 106 scenes are unlabeled\n",
    "unlabeled_scene_index = np.arange(106)\n",
    "# The scenes from 106 - 133 are labeled\n",
    "# You should devide the labeled_scene_index into two subsets (training and validation)\n",
    "labeled_scene_index = np.random.choice(np.arange(106, 134), size=28,replace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_inds = np.random.choice(labeled_scene_index,20,replace=False)\n",
    "val_inds = np.array([i for i in labeled_scene_index if i not in train_inds])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transform = torchvision.transforms.Compose([torchvision.transforms.Resize((256,256)),\n",
    "                                                  torchvision.transforms.RandomHorizontalFlip(),\n",
    "                                                  torchvision.transforms.ToTensor(),\n",
    "                                                  torchvision.transforms.Normalize(mean = [ 0.485, 0.456, 0.406 ],\n",
    "                                                                                   std = [ 0.229, 0.224, 0.225 ])])\n",
    "\n",
    "val_transform = torchvision.transforms.Compose([torchvision.transforms.Resize((256,256)),\n",
    "                                                torchvision.transforms.ToTensor(),\n",
    "                                                torchvision.transforms.Normalize(mean = [ 0.485, 0.456, 0.406 ],\n",
    "                                                                                  std = [ 0.229, 0.224, 0.225 ])])\n",
    "\n",
    "labeled_trainset = LabeledDataset(image_folder=image_folder,\n",
    "                                  annotation_file=annotation_csv,\n",
    "                                  scene_index=train_inds,\n",
    "                                  transform=train_transform,\n",
    "                                  extra_info=True\n",
    "                                 )\n",
    "\n",
    "labeled_valset = LabeledDataset(image_folder=image_folder,\n",
    "                                  annotation_file=annotation_csv,\n",
    "                                  scene_index=val_inds,\n",
    "                                  transform=val_transform,\n",
    "                                  extra_info=True\n",
    "                                 )\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(labeled_trainset, batch_size=1, shuffle=True, num_workers=2, collate_fn=collate_fn)\n",
    "valloader = torch.utils.data.DataLoader(labeled_valset, batch_size=1, shuffle=True, num_workers=2, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample, target, road_image, extra = iter(trainloader).next()\n",
    "print(torch.stack(sample).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(torchvision.utils.make_grid(sample[0], nrow=3).numpy().transpose(1, 2, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "color_list = ['b', 'g', 'orange', 'c', 'm', 'y', 'k', 'w', 'r']\n",
    "\n",
    "ax.imshow(road_image[0], cmap ='binary')\n",
    "ax.plot(400, 400, 'x', color=\"red\")\n",
    "for i, bb in enumerate(target[0]['bounding_box']):\n",
    "    # You can check the implementation of the draw box to understand how it works \n",
    "    draw_box(ax, bb, color=color_list[target[0]['category'][i]])   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "category_map = {'other_vehicle': 0, 'bicycle': 1, 'car': 2, 'pedestrian': 3, 'truck': 4,\n",
    "                'bus': 5, 'motorcycle': 6, 'emergency_vehicle': 7, 'animal': 8}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNet(nn.Module):\n",
    "    def ConvBlock(self, in_channels, out_channels, kernel_size = 3, stride = 1, padding = 1, use_bias = False):\n",
    "        block = nn.Sequential(nn.Conv2d(in_channels, out_channels, kernel_size, \n",
    "                                        stride, padding, bias = use_bias),\n",
    "                              nn.BatchNorm2d(out_channels),\n",
    "                              nn.ReLU(True)\n",
    "                             )\n",
    "        return block\n",
    "    \n",
    "    def Bridge(self, in_channels, out_channels):\n",
    "        bridge = nn.Sequential(self.ConvBlock(in_channels, out_channels),\n",
    "                               self.ConvBlock(out_channels, out_channels)\n",
    "                              )\n",
    "        return bridge\n",
    "    \n",
    "    def UpsampleBlock(self, in_channels, out_channels, use_bias=False):\n",
    "        upsample = nn.Sequential(nn.ConvTranspose2d(in_channels, out_channels, kernel_size=2, stride=2, bias=use_bias),\n",
    "                                 nn.BatchNorm2d(out_channels),\n",
    "                                 nn.ReLU(True))\n",
    "        return upsample\n",
    "        \n",
    "    def UpsampleConv(self, in_channels, out_channels):\n",
    "        upsample_conv = nn.Sequential(self.ConvBlock(in_channels, out_channels),\n",
    "                                      self.ConvBlock(out_channels, out_channels))    \n",
    "        return upsample_conv\n",
    "        \n",
    "        \n",
    "    def __init__(self, num_classes, output_size, encoder='resnet18', pretrained = False, depth = 6):\n",
    "        '''\n",
    "        num_classes: Number of channels/classes for segmentation\n",
    "        output_size: Final output size of the image (H*H)\n",
    "        encoder: Supports resnet18 and resnet50 architectures\n",
    "        pretrained: For loading pretrained resnet models as encoders\n",
    "        '''\n",
    "        super(UNet,self).__init__()  \n",
    "        self.depth = depth\n",
    "        self.num_classes = num_classes\n",
    "        self.output_size = output_size\n",
    "        \n",
    "        self.resnet = torchvision.models.resnet50(pretrained=pretrained) if encoder == \"resnet50\" else \\\n",
    "                                                    torchvision.models.resnet18(pretrained=pretrained)\n",
    "        self.resnet_layers = list(self.resnet.children())\n",
    "        self.n = 2048 if encoder == \"resnet50\" else 512\n",
    "        \n",
    "        self.input_block = nn.Sequential(*self.resnet_layers)[:3]\n",
    "        self.input_pool = self.resnet_layers[3]\n",
    "        self.down_blocks = nn.ModuleList([i for i in self.resnet_layers if isinstance(i, nn.Sequential)])\n",
    "\n",
    "        self.bridge = self.Bridge(self.n, self.n)\n",
    "        \n",
    "        self.up_blocks = nn.ModuleList([self.UpsampleBlock(self.n,self.n//2)[0],\n",
    "                                        self.UpsampleBlock(self.n//2,self.n//4)[0],\n",
    "                                        self.UpsampleBlock(self.n//4,self.n//8)[0],\n",
    "                                        self.UpsampleBlock(self.n//8,self.n//16)[0],\n",
    "                                        self.UpsampleBlock(self.n//16,self.n//32)[0]])\n",
    "        \n",
    "        self.up_conv = nn.ModuleList([self.UpsampleConv(self.n,self.n//2),\n",
    "                                      self.UpsampleConv(self.n//2,self.n//4),\n",
    "                                      self.UpsampleConv(self.n//4,self.n//8),\n",
    "                                      self.UpsampleConv(self.n//16 + 64,self.n//16),\n",
    "                                      self.UpsampleConv(self.n//32 + 3,self.n//32)])\n",
    "        \n",
    "        self.final_upsample_1 = self.UpsampleBlock(self.n//32,self.n//64)\n",
    "        self.final_upsample_2 = self.UpsampleBlock(self.n//64,self.num_classes)\n",
    "        \n",
    "        self.final_pooling = nn.AdaptiveMaxPool2d(output_size=self.output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        skip_conn = {\"layer_0\": x}\n",
    "        x = self.input_block(x)\n",
    "        skip_conn[f\"layer_1\"] = x\n",
    "        x = self.input_pool(x)\n",
    "\n",
    "        for i, block in enumerate(self.down_blocks, 2):\n",
    "            x = block(x)\n",
    "            if i != (self.depth - 1):\n",
    "                skip_conn[f\"layer_{i}\"] = x\n",
    "            \n",
    "        x = self.bridge(x)\n",
    "        x = torch.sum(x,dim=0)\n",
    "        x = x.repeat((6,1,1,1))\n",
    "\n",
    "        for i, block in enumerate(self.up_blocks):\n",
    "            key = f\"layer_{self.depth - i - 2}\"            \n",
    "            x = block(x)\n",
    "            x = torch.cat([x, skip_conn[key]],1)\n",
    "            x = self.up_conv[i](x)\n",
    "\n",
    "        del skip_conn\n",
    "        \n",
    "        x = torch.sum(x,dim=0)\n",
    "        x = self.final_upsample_1(x.unsqueeze(0))\n",
    "        x = self.final_upsample_2(x)\n",
    "        x = self.final_pooling(x)\n",
    "\n",
    "        return x.view(-1,self.output_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = UNet(num_classes=1, output_size=800, encoder=\"resnet18\", pretrained = False).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 1e-2\n",
    "momentum = 0.5\n",
    "num_epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(),lr=lr,momentum=momentum)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_ts_road_map(road_map1, road_map2):\n",
    "    tp = (road_map1 * road_map2).sum()\n",
    "\n",
    "    return tp * 1.0 / (road_map1.sum() + road_map2.sum() - tp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dice_loss(pred,truth):\n",
    "    return 1 - (2*torch.sum(pred*truth))/(torch.sum(pred*pred) + torch.sum(truth*truth))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    train_loss = 0\n",
    "    model.train()\n",
    "    for i, (sample, _, road_image, _) in enumerate(tqdm(trainloader)):        \n",
    "        \n",
    "        sample, road_image = sample[0].to(device), road_image[0].float().to(device)\n",
    "        \n",
    "        model.zero_grad()\n",
    "        out = model(sample)\n",
    "        loss = criterion(out, road_image) + dice_loss(out, road_image)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += loss.item()\n",
    "        if (i+1)%200 == 0:\n",
    "            print(\"Epoch: {} | Iter: {} | Train loss: {}\".format(epoch+1, i+1, train_loss/(i+1)))\n",
    "    \n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    val_ts = []\n",
    "    with torch.no_grad():        \n",
    "        for i, (sample, _, road_image, _) in enumerate(tqdm(valloader)):\n",
    "            sample, road_image = sample[0].to(device), road_image[0].float().to(device)\n",
    "            out = model(sample)\n",
    "            loss = criterion(out, road_image) + dice_loss(out, road_image)\n",
    "            val_loss += loss.item()\n",
    "            val_ts.append(compute_ts_road_map(out,road_image).item())\n",
    "\n",
    "        print(\"Epoch: {} | Val loss: {} | Val TS: {}\".format(epoch+1,val_loss/len(valloader.dataset), np.mean(val_ts)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample, target, road_image, extra = iter(trainloader).next()\n",
    "out = model(sample[0].to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(8,8))\n",
    "fig.add_subplot(1,2,1)\n",
    "plt.imshow((torch.sigmoid(out)>=0.5).float().detach().cpu().numpy(),cmap='binary')\n",
    "plt.title(\"Prediction\")\n",
    "\n",
    "fig.add_subplot(1,2,2)\n",
    "plt.imshow(road_image[0].cpu().numpy(),cmap='binary')\n",
    "plt.title(\"Ground Truth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
