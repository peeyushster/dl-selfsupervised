{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final competition of Deep Learning 2020 Spring\n",
    "Traffic environment semi-supervised Learning Contest\n",
    "\n",
    "## Goals\n",
    "The objective is to train a model using images captured by six different cameras attached to the same car to generate a top down view of the surrounding area. The performance of the model will be evaluated by (1) the ability of detecting objects (like car, trucks, bicycles, etc.) and (2) the ability to draw the road map layout.\n",
    "\n",
    "## Data\n",
    "You will be given two sets of data:\n",
    "\n",
    " 1. Unlabeled set: just images\n",
    " 2. Labeled set: images and the labels(bounding box and road map layout)\n",
    "\n",
    "This notebook will help you understand the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "matplotlib.rcParams['figure.figsize'] = [5, 5]\n",
    "matplotlib.rcParams['figure.dpi'] = 200\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "\n",
    "from data_helper import UnlabeledDataset, LabeledDataset\n",
    "from helper import collate_fn, draw_box\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from torchvision import transforms\n",
    "from helper import compute_ats_bounding_boxes, compute_ts_road_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(0)\n",
    "np.random.seed(0)\n",
    "torch.manual_seed(0);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All the images are saved in image_folder\n",
    "# All the labels are saved in the annotation_csv file\n",
    "image_folder = '../data'\n",
    "annotation_csv = '../data/annotation.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "annotation.csv\tscene_115  scene_133  scene_31\tscene_5   scene_68  scene_86\r\n",
      "scene_0\t\tscene_116  scene_14   scene_32\tscene_50  scene_69  scene_87\r\n",
      "scene_1\t\tscene_117  scene_15   scene_33\tscene_51  scene_7   scene_88\r\n",
      "scene_10\tscene_118  scene_16   scene_34\tscene_52  scene_70  scene_89\r\n",
      "scene_100\tscene_119  scene_17   scene_35\tscene_53  scene_71  scene_9\r\n",
      "scene_101\tscene_12   scene_18   scene_36\tscene_54  scene_72  scene_90\r\n",
      "scene_102\tscene_120  scene_19   scene_37\tscene_55  scene_73  scene_91\r\n",
      "scene_103\tscene_121  scene_2    scene_38\tscene_56  scene_74  scene_92\r\n",
      "scene_104\tscene_122  scene_20   scene_39\tscene_57  scene_75  scene_93\r\n",
      "scene_105\tscene_123  scene_21   scene_4\tscene_58  scene_76  scene_94\r\n",
      "scene_106\tscene_124  scene_22   scene_40\tscene_59  scene_77  scene_95\r\n",
      "scene_107\tscene_125  scene_23   scene_41\tscene_6   scene_78  scene_96\r\n",
      "scene_108\tscene_126  scene_24   scene_42\tscene_60  scene_79  scene_97\r\n",
      "scene_109\tscene_127  scene_25   scene_43\tscene_61  scene_8   scene_98\r\n",
      "scene_11\tscene_128  scene_26   scene_44\tscene_62  scene_80  scene_99\r\n",
      "scene_110\tscene_129  scene_27   scene_45\tscene_63  scene_81\r\n",
      "scene_111\tscene_13   scene_28   scene_46\tscene_64  scene_82\r\n",
      "scene_112\tscene_130  scene_29   scene_47\tscene_65  scene_83\r\n",
      "scene_113\tscene_131  scene_3    scene_48\tscene_66  scene_84\r\n",
      "scene_114\tscene_132  scene_30   scene_49\tscene_67  scene_85\r\n"
     ]
    }
   ],
   "source": [
    "## scence\n",
    "!ls ../data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample_0    sample_114\tsample_18  sample_34  sample_50  sample_67  sample_83\r\n",
      "sample_1    sample_115\tsample_19  sample_35  sample_51  sample_68  sample_84\r\n",
      "sample_10   sample_116\tsample_2   sample_36  sample_52  sample_69  sample_85\r\n",
      "sample_100  sample_117\tsample_20  sample_37  sample_53  sample_7   sample_86\r\n",
      "sample_101  sample_118\tsample_21  sample_38  sample_54  sample_70  sample_87\r\n",
      "sample_102  sample_119\tsample_22  sample_39  sample_55  sample_71  sample_88\r\n",
      "sample_103  sample_12\tsample_23  sample_4   sample_56  sample_72  sample_89\r\n",
      "sample_104  sample_120\tsample_24  sample_40  sample_57  sample_73  sample_9\r\n",
      "sample_105  sample_121\tsample_25  sample_41  sample_58  sample_74  sample_90\r\n",
      "sample_106  sample_122\tsample_26  sample_42  sample_59  sample_75  sample_91\r\n",
      "sample_107  sample_123\tsample_27  sample_43  sample_6\t sample_76  sample_92\r\n",
      "sample_108  sample_124\tsample_28  sample_44  sample_60  sample_77  sample_93\r\n",
      "sample_109  sample_125\tsample_29  sample_45  sample_61  sample_78  sample_94\r\n",
      "sample_11   sample_13\tsample_3   sample_46  sample_62  sample_79  sample_95\r\n",
      "sample_110  sample_14\tsample_30  sample_47  sample_63  sample_8   sample_96\r\n",
      "sample_111  sample_15\tsample_31  sample_48  sample_64  sample_80  sample_97\r\n",
      "sample_112  sample_16\tsample_32  sample_49  sample_65  sample_81  sample_98\r\n",
      "sample_113  sample_17\tsample_33  sample_5   sample_66  sample_82  sample_99\r\n"
     ]
    }
   ],
   "source": [
    "!ls ../data/scene_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CAM_BACK.jpeg\t    CAM_BACK_RIGHT.jpeg  CAM_FRONT_LEFT.jpeg\r\n",
      "CAM_BACK_LEFT.jpeg  CAM_FRONT.jpeg\t CAM_FRONT_RIGHT.jpeg\r\n"
     ]
    }
   ],
   "source": [
    "!ls ../data/scene_0/sample_0/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Labeled dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AddGaussianNoise(object):\n",
    "    def __init__(self, mean=0., std=1.):\n",
    "        self.std = std\n",
    "        self.mean = mean\n",
    "        \n",
    "    def __call__(self, tensor):\n",
    "        return tensor + torch.randn(tensor.size()) * self.std + self.mean\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__ + '(mean={0}, std={1})'.format(self.mean, self.std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled_scene_index = np.arange(106, 134)\n",
    "train_inds = np.random.choice(labeled_scene_index,20,replace=False)\n",
    "val_inds = np.array([i for i in labeled_scene_index if i not in train_inds])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The labeled dataset can only be retrieved by sample.\n",
    "# And all the returned data are tuple of tensors, since bounding boxes may have different size\n",
    "# You can choose whether the loader returns the extra_info. It is optional. You don't have to use it.\n",
    "\n",
    "transform_train = transforms.Compose([torchvision.transforms.Resize((256,256)),\n",
    "                                transforms.ToTensor(), \n",
    "                                transforms.Normalize([ 0.485, 0.456, 0.406 ],[ 0.229, 0.224, 0.225 ]), \n",
    "                                AddGaussianNoise(0., 0.1)])\n",
    "transform_val = transforms.Compose([torchvision.transforms.Resize((256,256)),\n",
    "                                transforms.ToTensor(), \n",
    "                                transforms.Normalize([ 0.485, 0.456, 0.406 ],[ 0.229, 0.224, 0.225 ])])\n",
    "\n",
    "labeled_trainset = LabeledDataset(image_folder=image_folder,\n",
    "                                  annotation_file=annotation_csv,\n",
    "                                  scene_index=train_inds,\n",
    "                                  transform=transform_train,\n",
    "                                  extra_info=True\n",
    "                                 )\n",
    "labeled_valset = LabeledDataset(image_folder=image_folder,\n",
    "                                  annotation_file=annotation_csv,\n",
    "                                  scene_index=val_inds,\n",
    "                                  transform=transform_val,\n",
    "                                  extra_info=True\n",
    "                                 )\n",
    "trainloader = torch.utils.data.DataLoader(labeled_trainset, batch_size=2, shuffle=True, num_workers=2, collate_fn=collate_fn)\n",
    "valloader = torch.utils.data.DataLoader(labeled_valset, batch_size=1,shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2520"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(labeled_trainset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 6, 3, 256, 256])\n"
     ]
    }
   ],
   "source": [
    "sample, target, road_image, extra = iter(trainloader).next()\n",
    "print(torch.stack(sample).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.models as models\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNet(nn.Module):\n",
    "    def ConvBlock(self, in_channels, out_channels, kernel_size = 3, stride = 1, padding = 1, use_bias = False):\n",
    "        block = nn.Sequential(nn.Conv2d(in_channels, out_channels, kernel_size, \n",
    "                                        stride, padding, bias = use_bias),\n",
    "                              nn.BatchNorm2d(out_channels),\n",
    "                              nn.ReLU(True)\n",
    "                             )\n",
    "        return block\n",
    "    \n",
    "#     def Bridge(self, in_channels, out_channels):\n",
    "#         bridge = nn.Sequential(self.ConvBlock(in_channels, out_channels),\n",
    "#                                self.ConvBlock(out_channels, out_channels)\n",
    "#                               )\n",
    "#         return bridge\n",
    "    \n",
    "    def UpsampleBlock(self, in_channels, out_channels, use_bias=False):\n",
    "        upsample = nn.Sequential(nn.ConvTranspose2d(in_channels, out_channels, kernel_size=2, stride=2, bias=use_bias),\n",
    "                                 nn.BatchNorm2d(out_channels),\n",
    "                                 nn.ReLU(True))\n",
    "        return upsample\n",
    "        \n",
    "    def UpsampleConv(self, in_channels, out_channels):\n",
    "        upsample_conv = nn.Sequential(self.ConvBlock(in_channels, out_channels),\n",
    "                                      self.ConvBlock(out_channels, out_channels))    \n",
    "        return upsample_conv\n",
    "        \n",
    "        \n",
    "    def __init__(self, num_classes, output_size, encoder='resnet18', pretrained = False, depth = 6):\n",
    "        '''\n",
    "        num_classes: Number of channels/classes for segmentation\n",
    "        output_size: Final output size of the image (H*H)\n",
    "        encoder: Supports resnet18 and resnet50 architectures\n",
    "        pretrained: For loading pretrained resnet models as encoders\n",
    "        '''\n",
    "        super(UNet,self).__init__()  \n",
    "        self.depth = depth\n",
    "        self.num_classes = num_classes\n",
    "        self.output_size = output_size\n",
    "        \n",
    "        self.resnet = torchvision.models.resnet50(pretrained=pretrained) if encoder == \"resnet50\" else \\\n",
    "                                                    torchvision.models.resnet18(pretrained=pretrained)\n",
    "        self.resnet_layers = list(self.resnet.children())\n",
    "        self.n = 2048 if encoder == \"resnet50\" else 512\n",
    "        \n",
    "        self.input_block = nn.Sequential(*self.resnet_layers)[:3]\n",
    "        self.input_block[0] = nn.Conv2d(18, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
    "        self.input_pool = self.resnet_layers[3]\n",
    "        self.down_blocks = nn.ModuleList([i for i in self.resnet_layers if isinstance(i, nn.Sequential)])\n",
    "\n",
    "        self.bridge_1_mu = self.ConvBlock(self.n, self.n)\n",
    "        self.bridge_1_var = self.ConvBlock(self.n, self.n)\n",
    "        self.bridge_2 = self.ConvBlock(self.n, self.n)\n",
    "        \n",
    "        \n",
    "        self.up_blocks = nn.ModuleList([self.UpsampleBlock(self.n,self.n//2)[0],\n",
    "                                        self.UpsampleBlock(self.n//2,self.n//4)[0],\n",
    "                                        self.UpsampleBlock(self.n//4,self.n//8)[0],\n",
    "                                        self.UpsampleBlock(self.n//8,self.n//16)[0],\n",
    "                                        self.UpsampleBlock(self.n//16,self.n//32)[0]])\n",
    "        \n",
    "        self.up_conv = nn.ModuleList([self.UpsampleConv(self.n,self.n//2),\n",
    "                                      self.UpsampleConv(self.n//2,self.n//4),\n",
    "                                      self.UpsampleConv(self.n//4,self.n//8),\n",
    "                                      self.UpsampleConv(self.n//16 + 64,self.n//16),\n",
    "                                      self.UpsampleConv(self.n//32 + 18,self.n//32)])\n",
    "        \n",
    "        self.final_upsample_1 = self.UpsampleBlock(self.n//32,self.n//64)\n",
    "        self.final_upsample_2 = self.UpsampleBlock(self.n//64,self.num_classes)\n",
    "        \n",
    "        self.final_pooling = nn.AdaptiveMaxPool2d(output_size=self.output_size)\n",
    "    \n",
    "    def reparameterize(self,mu,logvar):\n",
    "        if self.training:\n",
    "            std = logvar.mul(0.5).exp_()\n",
    "            eps = std.data.new(std.size()).normal_()\n",
    "            return eps.mul(std).add_(mu)\n",
    "        else:\n",
    "            return mu\n",
    "\n",
    "    def forward(self, x):\n",
    "        skip_conn = {\"layer_0\": x}\n",
    "        x = self.input_block(x)\n",
    "        skip_conn[f\"layer_1\"] = x\n",
    "        x = self.input_pool(x)\n",
    "\n",
    "        for i, block in enumerate(self.down_blocks, 2):\n",
    "            x = block(x)\n",
    "            if i != (self.depth - 1):\n",
    "                skip_conn[f\"layer_{i}\"] = x\n",
    "            \n",
    "        mu = self.bridge_1_mu(x)\n",
    "        logvar = self.bridge_1_var(x)\n",
    "        x = self.reparameterize(mu,logvar)\n",
    "        x = self.bridge_2(x)\n",
    "        \n",
    "        #print(x.shape)\n",
    "    \n",
    "        #x = torch.sum(x,dim=0)\n",
    "        #x = x.repeat((6,1,1,1))\n",
    "\n",
    "        for i, block in enumerate(self.up_blocks):\n",
    "            key = f\"layer_{self.depth - i - 2}\"            \n",
    "            x = block(x)\n",
    "            #print(x.shape)\n",
    "            x = torch.cat([x, skip_conn[key]],1)\n",
    "            #print(x.shape)\n",
    "            x = self.up_conv[i](x)\n",
    "\n",
    "        del skip_conn\n",
    "        \n",
    "        #x = torch.sum(x,dim=0)\n",
    "        x = self.final_upsample_1(x)\n",
    "        x = self.final_upsample_2(x)\n",
    "        x = self.final_pooling(x)\n",
    "\n",
    "        return x,mu,logvar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_function(x_hat, x, mu, logvar):\n",
    "    #x_hat_sig = torch.sigmoid(x_hat)\n",
    "    DICE =  1 - (2*torch.sum(x_hat*x))/(torch.sum(x*x) + torch.sum(x_hat*x_hat))\n",
    "    MSE = F.mse_loss(\n",
    "        x_hat, x, reduction='mean'\n",
    "    )\n",
    "    KLD = (0.5 * torch.sum(logvar.exp() - logvar - 1 + mu.pow(2)))/mu.numel()\n",
    "    #print(DICE,MSE,KLD)\n",
    "    return (DICE+MSE + KLD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:4')\n",
    "model = model = UNet(num_classes=1, output_size=800, encoder=\"resnet18\", pretrained = False).to(device)\n",
    "lr = 1e-2\n",
    "momentum = 0.75\n",
    "optimizer = torch.optim.SGD(model.parameters(),lr=lr,momentum=momentum)\n",
    "criterion = loss_function\n",
    "#scheduler = StepLR(optimizer, step_size=1, gamma=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation(model,val_loader,device):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        tse = 0\n",
    "        total = 0\n",
    "        for i,(sample, target, road_image,extra) in enumerate(val_loader):\n",
    "            total+=1  \n",
    "            input_img = sample.to(device)\n",
    "            #input_img = torch.stack(sample).to(device)\n",
    "            batch_size,height,width = input_img.shape[0],input_img.shape[-2],input_img.shape[-1]\n",
    "            output,mu,logvar = model(input_img.view(batch_size,-1,height,width))\n",
    "            one_idx = output>0.5\n",
    "            output[one_idx] = 1.0\n",
    "            output[~one_idx] = 0.0\n",
    "            predicted_road_map = (output.squeeze(1)).float()\n",
    "            #print(road_image.shape)\n",
    "            tse+=compute_ts_road_map(predicted_road_map, road_image.float().to(device))\n",
    "        #print(tse,total)\n",
    "        return (tse/total).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model,train_loader,val_loader,criterion,optimizer,device,num_epochs = 10):\n",
    "    max_val = -10\n",
    "    #model.load_state_dict(torch.load('model_unet_sgd.pth'))\n",
    "    for epoch in range(1,num_epochs+1):\n",
    "        avg_train_loss = 0\n",
    "        for i,(sample, target, road_image, extra) in enumerate(train_loader):\n",
    "            input_img = torch.stack(sample).to(device)\n",
    "            batch_size,height,width = input_img.shape[0],input_img.shape[-2],input_img.shape[-1]\n",
    "            optimizer.zero_grad()\n",
    "            output,mu,logvar = model(input_img.view(batch_size,-1,height,width))\n",
    "            #print(output.shape)\n",
    "            loss = criterion(output.squeeze(1),torch.stack(road_image).float().to(device),mu,logvar)\n",
    "            #print(loss)\n",
    "            avg_train_loss+=loss.item()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            if (i+1)%420==0:\n",
    "                val_tse = validation(model,val_loader,device)\n",
    "                print('epoch: {} | step: {}/{} | train loss: {} | val tse: {}'.format(epoch,\n",
    "                                                                                      (i+1)//420,\n",
    "                                                                                      len(train_loader)//420,\n",
    "                                                                                      round(avg_train_loss/420,2),\n",
    "                                                                                      round(val_tse,2)))\n",
    "                model.train()\n",
    "                avg_train_loss = 0\n",
    "                if val_tse>max_val:\n",
    "                    print('--Saving--')\n",
    "                    torch.save(model.state_dict(),'model_unet_sgd_mse_(1,1).pth')\n",
    "                    max_val = val_tse\n",
    "        #scheduler.step()\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1 | step: 1/3 | train loss: 0.71 | val tse: 0.56\n",
      "--Saving--\n",
      "epoch: 1 | step: 2/3 | train loss: 0.44 | val tse: 0.64\n",
      "--Saving--\n"
     ]
    }
   ],
   "source": [
    "train(model,trainloader,valloader,criterion,optimizer,device )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('model_unet_sgd_mse.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([6, 3, 256, 306])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labeled_valset[10][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = iter(valloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1.],\n",
       "        [0., 1.]])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.tensor([[1.5,1],[0.25,0.51]])\n",
    "b = a>0.5\n",
    "a[b] = 1\n",
    "a[~b]=0\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    sample, target, road_image, extra = labeled_trainset[2150]\n",
    "    input_img = sample.to(device)\n",
    "            #input_img = torch.stack(sample).to(device)\n",
    "    batch_size,height,width = 1,input_img.shape[-2],input_img.shape[-1]\n",
    "    output,mu,logvar = model(input_img.view(batch_size,-1,height,width))\n",
    "    one_idx = output>0.5\n",
    "    output[one_idx] = 1.0\n",
    "    output[~one_idx] = 0.0\n",
    "    predicted_road_map = (output.squeeze(1)).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f1853978ed0>"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(predicted_road_map[0].cpu().data.numpy(),cmap='binary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f18538deed0>"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(road_image, cmap='binary')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extra Info\n",
    "\n",
    "There is some extra information you can use in your model, but it is optional."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize the bounding box"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# The center of image is 400 * 400\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "color_list = ['b', 'g', 'orange', 'c', 'm', 'y', 'k', 'w', 'r']\n",
    "\n",
    "ax.imshow(road_image[0], cmap ='binary');\n",
    "\n",
    "# The ego car position\n",
    "ax.plot(400, 400, 'x', color=\"red\")\n",
    "\n",
    "for i, bb in enumerate(target[0]['bounding_box']):\n",
    "    # You can check the implementation of the draw box to understand how it works \n",
    "    draw_box(ax, bb, color=color_list[target[0]['category'][i]])    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation\n",
    "During the whole competition, you have three submission deadlines. The dates will be announced on Piazza. You will have to fill up the template 'data_loader.py' for evaluation. (see the comment inside data_loader.py' for more information)\n",
    "\n",
    "There will be two leaderboards for the competition:\n",
    "The leaderboard for binary road map.\n",
    "We will evaluate your model's performance by using the average threat score (TS) across the test set:\n",
    "$$\\text{TS} = \\frac{\\text{TP}}{\\text{TP} + \\text{FP} + \\text{FN}}$$\n",
    "The leaderboard for object detection:\n",
    "We will evaluate your model's performance for object detection by using the average mean threat score at different intersection over union (IoU) thresholds.\n",
    "There will be five different thresholds (0.5, 0.6, 0.7, 0.8, 0.9). For each thresholds, we will calculate the threat score. The final score will be a weighted average of all the threat scores:\n",
    "$$\\text{Final Score} = \\sum_t \\frac{1}{t} \\cdot \\frac{\\text{TP}(t)}{\\text{TP}(t) + \\text{FP}(t) + \\text{FN}(t)}$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
