{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm, trange\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "matplotlib.rcParams['figure.figsize'] = [12,12]\n",
    "matplotlib.rcParams['figure.dpi'] = 200\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "\n",
    "from data_helper import LabeledDataset,UnlabeledDataset\n",
    "from helper import collate_fn, draw_box\n",
    "\n",
    "import itertools\n",
    "from scipy.spatial.distance import cdist\n",
    "from helper import compute_ats_bounding_boxes, compute_ts_road_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(0)\n",
    "np.random.seed(0)\n",
    "torch.manual_seed(0);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All the images are saved in image_folder\n",
    "# All the labels are saved in the annotation_csv file\n",
    "image_folder = '../data'\n",
    "annotation_csv = '../data/annotation.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You shouldn't change the unlabeled_scene_index\n",
    "# The first 106 scenes are unlabeled\n",
    "unlabeled_scene_index = np.arange(106)\n",
    "# The scenes from 106 - 133 are labeled\n",
    "# You should devide the labeled_scene_index into two subsets (training and validation)\n",
    "labeled_scene_index = np.arange(106, 134)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Labeled Dataloader "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AddGaussianNoise(object):\n",
    "    def __init__(self, mean=0., std=1.):\n",
    "        self.std = std\n",
    "        self.mean = mean\n",
    "        \n",
    "    def __call__(self, tensor):\n",
    "        return tensor + torch.randn(tensor.size()) * self.std + self.mean\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__ + '(mean={0}, std={1})'.format(self.mean, self.std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled_scene_index = np.arange(106, 134)\n",
    "train_inds = labeled_scene_index[:25]\n",
    "val_inds = labeled_scene_index[25:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_train = transforms.Compose([torchvision.transforms.Resize((256,256)),\n",
    "                                      torchvision.transforms.RandomApply([torchvision.transforms.ColorJitter(brightness=[0,1], contrast=[0,1], saturation=[0,1], hue=[-0.5,0.5])],p=0.5),\n",
    "                                      torchvision.transforms.RandomGrayscale(p=0.5),\n",
    "                                      torchvision.transforms.RandomHorizontalFlip(p=0.5), \n",
    "                                      transforms.ToTensor(), \n",
    "                                      #transforms.Normalize([ 0.485, 0.456, 0.406 ],[ 0.229, 0.224, 0.225 ]),\n",
    "                                      AddGaussianNoise(0., 0.1),\n",
    "                                    ])\n",
    "transform_val = transforms.Compose([torchvision.transforms.Resize((256,256)),\n",
    "                                transforms.ToTensor(), \n",
    "                                #transforms.Normalize([ 0.485, 0.456, 0.406 ],[ 0.229, 0.224, 0.225 ])\n",
    "                                   ])\n",
    "\n",
    "labeled_trainset = LabeledDataset(image_folder=image_folder,\n",
    "                                  annotation_file=annotation_csv,\n",
    "                                  scene_index=train_inds,\n",
    "                                  transform=transform_train,\n",
    "                                  extra_info=True\n",
    "                                 )\n",
    "labeled_valset = LabeledDataset(image_folder=image_folder,\n",
    "                                  annotation_file=annotation_csv,\n",
    "                                  scene_index=val_inds,\n",
    "                                  transform=transform_val,\n",
    "                                  extra_info=True\n",
    "                                 )\n",
    "labeled_trainloader = torch.utils.data.DataLoader(labeled_trainset, batch_size=2, shuffle=True, num_workers=2, collate_fn=collate_fn)\n",
    "labeled_valloader = torch.utils.data.DataLoader(labeled_valset, batch_size=1,shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=1)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def ConvBlock(self, in_channels, out_channels, kernel_size = 3, stride = 1, padding = 1, use_bias = False):\n",
    "        block = nn.Sequential(nn.Conv2d(in_channels, out_channels, kernel_size, \n",
    "                                        stride, padding, bias = use_bias),\n",
    "                              nn.BatchNorm2d(out_channels),\n",
    "                              nn.ReLU(True)\n",
    "                             )\n",
    "        return block\n",
    "    \n",
    "    def Bridge(self, in_channels, out_channels):\n",
    "        bridge = nn.Sequential(self.ConvBlock(in_channels, out_channels),\n",
    "                               self.ConvBlock(out_channels, out_channels)\n",
    "                              )\n",
    "        return bridge\n",
    "        \n",
    "        \n",
    "    def __init__(self, encoder='resnet34', pretrained = False, depth = 6):\n",
    "        '''\n",
    "        num_classes: Number of channels/classes for segmentation\n",
    "        output_size: Final output size of the image (H*H)\n",
    "        encoder: Supports resnet18, resnet 34 and resnet50 architectures\n",
    "        pretrained: For loading a pretrained resnet model as encoder\n",
    "        '''\n",
    "        super(Encoder,self).__init__()  \n",
    "        self.depth = depth        \n",
    "        self.resnet = torchvision.models.resnet50(pretrained=pretrained) if encoder == \"resnet50\" else\\\n",
    "                            torchvision.models.resnet34(pretrained=pretrained) if encoder == \"resnet34\" else\\\n",
    "                            torchvision.models.resnet18(pretrained=pretrained)\n",
    "        \n",
    "        self.resnet_layers = list(self.resnet.children())\n",
    "        self.n = 2048 if encoder == \"resnet50\" else 512\n",
    "        \n",
    "        self.input_block = nn.Sequential(*self.resnet_layers)[:3]\n",
    "        #self.input_block[0] = nn.Conv2d(18, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
    "        self.input_pool = self.resnet_layers[3]\n",
    "        self.down_blocks = nn.ModuleList([i for i in self.resnet_layers if isinstance(i, nn.Sequential)])\n",
    "\n",
    "        self.bridge_mu = self.Bridge(self.n, self.n)\n",
    "        self.bridge_logvar = self.Bridge(self.n, self.n)\n",
    "        \n",
    "    def reparameterize(self,mu,logvar):\n",
    "        if self.training:\n",
    "            std = logvar.mul(0.5).exp_()\n",
    "            eps = std.data.new(std.size()).normal_()\n",
    "            return eps.mul(std).add_(mu)\n",
    "        else:\n",
    "            return mu\n",
    "        \n",
    "    def forward(self,x):\n",
    "        B = x.shape[0]\n",
    "        x = x.transpose(0,1)\n",
    "        \n",
    "        mu_list = []\n",
    "        logvar_list = []\n",
    "        for i in range(6):\n",
    "            z = self.input_pool(self.input_block(x[i]))\n",
    "            for block in self.down_blocks:\n",
    "                z = block(z)\n",
    "            mu = self.bridge_mu(z)\n",
    "            logvar = self.bridge_logvar(z)\n",
    "            mu_list.append(mu)\n",
    "            logvar_list.append(logvar)\n",
    "#             h = self.pool(z)\n",
    "#             h = h.view([B,1,-1])\n",
    "#             h_list.append(h)\n",
    "            \n",
    "#         del h   \n",
    "        del mu\n",
    "        del logvar\n",
    "#         h = torch.cat(h_list,1)\n",
    "        mu = torch.cat(mu_list,1)\n",
    "        logvar = torch.cat(logvar_list,1)\n",
    "#         del h_list\n",
    "        del mu_list\n",
    "        del logvar_list\n",
    "        z = self.reparameterize(mu,logvar)\n",
    "        return z,mu,logvar #b,512*6,8,8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def ConvBlock(self, in_channels, out_channels, kernel_size = 3, stride = 1, padding = 1, use_bias = False):\n",
    "        block = nn.Sequential(nn.Conv2d(in_channels, out_channels, kernel_size, \n",
    "                                        stride, padding, bias = use_bias),\n",
    "                              nn.BatchNorm2d(out_channels),\n",
    "                              nn.ReLU(True)\n",
    "                             )\n",
    "        return block\n",
    "    \n",
    "    def Bridge(self, in_channels, out_channels):\n",
    "        bridge = nn.Sequential(self.ConvBlock(in_channels, out_channels),\n",
    "                               self.ConvBlock(out_channels, out_channels)\n",
    "                              )\n",
    "        return bridge\n",
    "    \n",
    "    def UpsampleBlock(self, in_channels, out_channels, use_bias=False):\n",
    "        upsample = nn.Sequential(nn.ConvTranspose2d(in_channels, out_channels, kernel_size=2, stride=2, bias=use_bias),\n",
    "                                 nn.BatchNorm2d(out_channels),\n",
    "                                 nn.ReLU(True))\n",
    "        return upsample\n",
    "        \n",
    "    def UpsampleConv(self, in_channels, out_channels):\n",
    "        upsample_conv = nn.Sequential(self.ConvBlock(in_channels, out_channels),\n",
    "                                      self.ConvBlock(out_channels, out_channels))    \n",
    "        return upsample_conv\n",
    "        \n",
    "        \n",
    "    def __init__(self, classes = 1, depth = 6, output_size=(800,800), input_channels=512):\n",
    "        '''\n",
    "        num_classes: Number of channels/classes for segmentation\n",
    "        output_size: Final output size of the image (H*H)\n",
    "        encoder: Supports resnet18, resnet 34 and resnet50 architectures\n",
    "        pretrained: For loading a pretrained resnet model as encoder\n",
    "        '''\n",
    "        super(Decoder,self).__init__()  \n",
    "        self.depth = depth        \n",
    "        self.num_classes = classes\n",
    "        self.output_size = output_size\n",
    "        self.n  = input_channels\n",
    "        \n",
    "        self.up_blocks = nn.ModuleList([self.UpsampleBlock(self.n,self.n//2)[0],\n",
    "                                        self.UpsampleBlock(self.n//2,self.n//4)[0],\n",
    "                                        self.UpsampleBlock(self.n//4,self.n//8)[0],\n",
    "                                        self.UpsampleBlock(self.n//8,self.n//16)[0],\n",
    "                                        self.UpsampleBlock(self.n//16,self.n//32)[0]])\n",
    "        \n",
    "        self.up_conv = nn.ModuleList([self.UpsampleConv(self.n//2,self.n//2),\n",
    "                                      self.UpsampleConv(self.n//4,self.n//4),\n",
    "                                      self.UpsampleConv(self.n//8,self.n//8),\n",
    "                                      self.UpsampleConv(self.n//16 ,self.n//16),\n",
    "                                      self.UpsampleConv(self.n//32,self.n//32)])\n",
    "        \n",
    "        self.final_upsample_1 = self.UpsampleBlock(self.n//32,self.n//64)\n",
    "        self.final_upsample_2 = self.UpsampleBlock(self.n//64,self.num_classes)[0]\n",
    "        \n",
    "        self.final_pooling = nn.AdaptiveMaxPool2d(output_size=self.output_size)\n",
    "        \n",
    "    def forward(self, z):\n",
    "        num_iters = z.shape[1]//self.n\n",
    "        x_list = []\n",
    "        for j in range(num_iters):\n",
    "            hidden = z[:,self.n*j:self.n*(j+1)]\n",
    "            for i, block in enumerate(self.up_blocks):          \n",
    "                hidden = block(hidden)\n",
    "                hidden = self.up_conv[i](hidden)\n",
    "            x = self.final_upsample_1(hidden)#.unsqueeze(0))\n",
    "            del hidden\n",
    "            x = self.final_upsample_2(x)\n",
    "            x = self.final_pooling(x)\n",
    "            x = x.view(-1,self.num_classes,self.output_size,self.output_size)\n",
    "            x_list.append(x)\n",
    "            del x\n",
    "            #x = torch.sigmoid(x)\n",
    "        return torch.stack(x_list,dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lr = 5e-5\n",
    "# momentum = 0.9\n",
    "# num_epochs = 50\n",
    "# weight_decay = 5e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#vae_decoder = Decoder(classes = 3, depth = 6, output_size=256, input_channels=512).to(device)\n",
    "road_decoder = Decoder(classes = 1, depth = 6, output_size=800, input_channels=512*6).to(device)\n",
    "encoder = Encoder(encoder='resnet18', pretrained = False).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_function_road(x_hat, x,mu,logvar):\n",
    "    #x_hat_sig = torch.sigmoid(x_hat)\n",
    "    #DICE =  1 - (2*torch.sum(x_hat_sig*x))/(torch.sum(x*x) + torch.sum(x_hat_sig*x_hat_sig))\n",
    "    #tp = (x_hat_sig * x).sum()\n",
    "    #ts = tp * 1.0 / (x_hat_sig.sum() + x.sum() - tp)\n",
    "    BCE = F.binary_cross_entropy_with_logits(\n",
    "        x_hat, x, reduction='mean'\n",
    "    )\n",
    "    KLD = (0.5 * torch.sum(logvar.exp() - logvar - 1 + mu.pow(2)))/mu.numel()\n",
    "    return BCE+KLD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def loss_function_vae(x_hat, x, mu, logvar):\n",
    "#     MSE = F.mse_loss(\n",
    "#         x_hat, x, reduction='mean'\n",
    "#     )\n",
    "#     KLD = (0.5 * torch.sum(logvar.exp() - logvar - 1 + mu.pow(2)))/mu.numel()\n",
    "#     #print(MSE,KLD)\n",
    "#     return MSE+5*KLD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#criterion_vae = loss_function_vae\n",
    "criterion_road = loss_function_road\n",
    "#vae_optimizer = torch.optim.Adam(vae_decoder.parameters(),lr=5e-5)\n",
    "encoder_optimizer = torch.optim.Adam(encoder.parameters(),lr=5e-5)\n",
    "road_optimizer = torch.optim.Adam(road_decoder.parameters(),lr=5e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation(encoder,decoder,valloader,device):\n",
    "    encoder.eval()\n",
    "    decoder.eval()\n",
    "    with torch.no_grad():\n",
    "        tse_road = 0.\n",
    "        total = 0.\n",
    "        for i,(sample, target, road_image,extra) in enumerate(valloader):\n",
    "            total+=1.0  \n",
    "            input_img = sample.to(device)\n",
    "            z,mu,logvar = encoder(input_img)\n",
    "            predicted_road = decoder(z)[:,0,0]\n",
    "            predicted_road_map = torch.sigmoid(predicted_road)>0.5\n",
    "            predicted_road_map = (predicted_road_map.squeeze(1)).float()\n",
    "            \n",
    "            tse_road+=compute_ts_road_map(predicted_road_map, road_image.float().to(device))\n",
    "            \n",
    "        return (tse_road/total).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def train(encoder, \n",
    "#           road_decoder, \n",
    "#           vae_decoder, \n",
    "#           labeled_trainloader, \n",
    "#           unlabeled_trainloader, \n",
    "#           valloader, \n",
    "#           criterion_vae, \n",
    "#           vae_optimizer, \n",
    "#           encoder_optimizer, \n",
    "#           road_optimizer,\n",
    "#           criterion_road, \n",
    "#           device=device, \n",
    "#           c_point=1, \n",
    "#           max_iter=400):\n",
    "#     labeled_iter = iter(labeled_trainloader)\n",
    "#     unlabeled_iter = iter(unlabeled_trainloader)\n",
    "#     unlabeled_training_loss = 0\n",
    "#     labeled_training_loss = 0\n",
    "#     threshold_val = 0.722\n",
    "#     for iterator in tqdm(range(1,max_iter+1)):\n",
    "        \n",
    "#         vae_optimizer.zero_grad() \n",
    "#         encoder_optimizer.zero_grad() \n",
    "#         road_optimizer.zero_grad()\n",
    "        \n",
    "#         try:\n",
    "#             unlabeled_img = next(unlabeled_iter)\n",
    "#         except:\n",
    "#             unlabeled_iter = iter(unlabeled_trainloader)\n",
    "#             unlabeled_img = next(unlabeled_iter,None)\n",
    "#         #print(unlabeled_img.shape)\n",
    "#         z,mu,logvar = encoder(unlabeled_img.to(device))\n",
    "#         #print(z.shape)\n",
    "#         imgs_predicted = vae_decoder(z)\n",
    "#         #print(imgs_predicted.shape)\n",
    "#         u_loss = criterion_vae(imgs_predicted, unlabeled_img.to(device),mu,logvar)\n",
    "#         unlabeled_training_loss+=u_loss.item()\n",
    "        \n",
    "#         l_loss = 0\n",
    "#         for j in range(5):\n",
    "#             try:\n",
    "#                 labeled_img,_,road_image,_ = next(labeled_iter)\n",
    "#             except:\n",
    "#                 labeled_iter = iter(labeled_trainloader)\n",
    "#                 labeled_img,_,road_image,_ = next(labeled_iter)\n",
    "#             #print(labeled_img.shape)\n",
    "#             labeled_img = torch.stack(labeled_img).to(device)\n",
    "#             #print(z.shape)\n",
    "#             z,_,_ = encoder(labeled_img)\n",
    "#             road_predicted = road_decoder(z)\n",
    "#             #print(road_predicted.shape)\n",
    "#             l_loss+= criterion_road(road_predicted[:,0,0], torch.stack(road_image).float().to(device))\n",
    "#         labeled_training_loss+=l_loss.item()\n",
    "\n",
    "#         loss = u_loss+4*l_loss\n",
    "#         loss.backward()\n",
    "        \n",
    "#         vae_optimizer.step() \n",
    "#         encoder_optimizer.step() \n",
    "#         road_optimizer.step()\n",
    "        \n",
    "#         if iterator%c_point==0:\n",
    "#             val_tse_road = validation(encoder,road_decoder,valloader,device)\n",
    "#             print('iterator: {}/{} | train loss labeled: {} | train loss unlabeled: {} | val ts road: {}'.format(iterator, \n",
    "#                                                                                                                  max_iter,\n",
    "#                                                                                                                  round(labeled_training_loss/c_point,2), \n",
    "#                                                                                                                  round(unlabeled_training_loss/c_point,2), \n",
    "#                                                                                                                  round(val_tse_road,3)))\n",
    "#             unlabeled_training_loss = 0\n",
    "#             labeled_training_loss = 0\n",
    "#             encoder.train()\n",
    "#             road_decoder.train()\n",
    "            \n",
    "#             if val_tse_road>threshold_val:\n",
    "#                 print('--Saving--')\n",
    "#                 torch.save(encoder.state_dict(),'ss/encoder.pth')\n",
    "#                 torch.save(road_decoder.state_dict(),'ss/road_decoder.pth')\n",
    "#                 torch.save(vae_decoder.state_dict(),'ss/vae_decoder.pth')\n",
    "#                 threshold_val = val_tse_road\n",
    "                \n",
    "                \n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train(encoder, \n",
    "#       road_decoder, \n",
    "#       vae_decoder, \n",
    "#       labeled_trainloader, \n",
    "#       unlabeled_trainloader, \n",
    "#       labeled_valloader, \n",
    "#       criterion_vae, \n",
    "#       vae_optimizer, \n",
    "#       encoder_optimizer, \n",
    "#       road_optimizer,\n",
    "#       criterion_road, \n",
    "#       device=device, \n",
    "#       c_point=20, \n",
    "#       max_iter=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def train(encoder, \n",
    "          road_decoder, \n",
    "          labeled_trainloader, \n",
    "          valloader,\n",
    "          encoder_optimizer, \n",
    "          road_optimizer,\n",
    "          criterion_road, \n",
    "          device=device, \n",
    "          c_point=1, \n",
    "          max_iter=400):\n",
    "    labeled_iter = iter(labeled_trainloader)\n",
    "    labeled_training_loss = 0\n",
    "    threshold_val = 0\n",
    "    encoder.train()\n",
    "    road_decoder.train()\n",
    "    #encoder.load_state_dict(torch.load('da/encoder.pth'))\n",
    "    #road_decoder.load_state_dict(torch.load('da/road_decoder.pth'))\n",
    "    for iterator in tqdm(range(1,max_iter+1)):\n",
    "        road_optimizer.zero_grad()\n",
    "        encoder_optimizer.zero_grad()\n",
    "        \n",
    "        try:\n",
    "            labeled_img,_,road_image,_ = next(labeled_iter)\n",
    "        except:\n",
    "            labeled_iter = iter(labeled_trainloader)\n",
    "            labeled_img,_,road_image,_ = next(labeled_iter)\n",
    "        #print(labeled_img.shape)\n",
    "        labeled_img = torch.stack(labeled_img).to(device)\n",
    "        #print(z.shape)\n",
    "        z,mu,logvar = encoder(labeled_img)\n",
    "        road_predicted = road_decoder(z)\n",
    "        #print(road_predicted.shape)\n",
    "        l_loss = criterion_road(road_predicted[:,0,0], torch.stack(road_image).float().to(device),mu,logvar)\n",
    "        labeled_training_loss+=l_loss.item()\n",
    "\n",
    "        loss = l_loss\n",
    "        loss.backward()\n",
    "        \n",
    "        road_optimizer.step()\n",
    "        encoder_optimizer.step()\n",
    "        \n",
    "        if iterator%c_point==0:\n",
    "            val_tse_road = validation(encoder,road_decoder,valloader,device)\n",
    "            print('iterator: {}/{} | train loss labeled: {} | val ts road: {}'.format(iterator, \n",
    "                                                                                     max_iter,\n",
    "                                                                                     round(labeled_training_loss/c_point,2), \n",
    "                                                                                     round(val_tse_road,3)))\n",
    "            \n",
    "            labeled_training_loss = 0\n",
    "            road_decoder.train()\n",
    "            encoder.train()\n",
    "            \n",
    "            if val_tse_road>threshold_val:\n",
    "                print('--Saving--')\n",
    "                torch.save(road_decoder.state_dict(),'da/road_decoder.pth')\n",
    "                torch.save(encoder.state_dict(),'da/encoder.pth')\n",
    "                threshold_val = val_tse_road\n",
    "                \n",
    "                \n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▍         | 49/1000 [00:12<03:51,  4.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iterator: 50/1000 | train loss labeled: 0.94 | val ts road: 0.568\n",
      "--Saving--\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|▉         | 99/1000 [00:49<03:40,  4.09it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iterator: 100/1000 | train loss labeled: 0.63 | val ts road: 0.706\n",
      "--Saving--\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▌        | 150/1000 [01:50<1:39:04,  6.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iterator: 150/1000 | train loss labeled: 0.57 | val ts road: 0.392\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|█▉        | 199/1000 [02:02<03:15,  4.10it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iterator: 200/1000 | train loss labeled: 0.55 | val ts road: 0.709\n",
      "--Saving--\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▍       | 249/1000 [02:38<03:03,  4.09it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iterator: 250/1000 | train loss labeled: 0.52 | val ts road: 0.716\n",
      "--Saving--\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 300/1000 [03:33<1:09:07,  5.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iterator: 300/1000 | train loss labeled: 0.49 | val ts road: 0.68\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|███▌      | 350/1000 [04:06<1:08:20,  6.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iterator: 350/1000 | train loss labeled: 0.52 | val ts road: 0.715\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|███▉      | 399/1000 [04:18<02:27,  4.08it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iterator: 400/1000 | train loss labeled: 0.44 | val ts road: 0.722\n",
      "--Saving--\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|████▍     | 449/1000 [04:56<02:15,  4.07it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iterator: 450/1000 | train loss labeled: 0.48 | val ts road: 0.723\n",
      "--Saving--\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 500/1000 [05:56<58:35,  7.03s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iterator: 500/1000 | train loss labeled: 0.46 | val ts road: 0.587\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|█████▌    | 550/1000 [06:32<55:47,  7.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iterator: 550/1000 | train loss labeled: 0.44 | val ts road: 0.715\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|█████▉    | 599/1000 [06:44<01:38,  4.07it/s]"
     ]
    }
   ],
   "source": [
    "train(encoder, \n",
    "          road_decoder, \n",
    "          labeled_trainloader, \n",
    "          labeled_valloader,\n",
    "          encoder_optimizer, \n",
    "          road_optimizer,\n",
    "          criterion_road, \n",
    "          device=device, \n",
    "          c_point=50, \n",
    "          max_iter=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
